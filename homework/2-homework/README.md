# Homework 2 (10 Points + 3 Bonus)

In your private repo called `eco395m-ml-student` and invite the TA and I. Make a folder called `2-homework`. For exercises containing programming, take the starter code (if there is any) and move it into the folder and solve. For written exercises (including mathematical ones) either submit the solution as a neatly written scan or picture or a markdown file (containing LaTeX). Inside a file called `README.md` in the homework folder, indicate which file, images etc. contain which solutions.

1. **Polynomial Regression Optimal Degree**: (2 points) In class, we looked at polynomial regression with 0, 2 and very large degree polynomial features. Using Scikit-learn (GridSearchCV and make_pipeline, etc.), on the advertising dataset, determine what degree is optimal. Show the degree, the train MSE and the test MSE.
2. **Tree Depth**: (1 point) Assuming no irreducible error in the dataset (i.e. there are no two observations that have the same feature values, but different target values), for a dataset of length N, what is the depth of the regression tree that will be formed with scikit-learn default hyperparameters?
3. **Log Targets and Tree/Forests**: In class, I argued that scaling of features does not make a difference to regression trees/forests. But, what happens when we transform the target? Let’s use Scikit-learn to explore.
   * a. (0 points) Load the data, dropping the columns that are not numeric (we’ll look at how to encode categorical data later in the course) and the rows for which the salary is missing.
   * b. (0 points) Split the dataset into a test and a train set using random_state=42
   * c. (0 points) Train a random forest regressor, with n_estimators=1000 to ensure stability.
   * d. (0 points) Train a transformed target regressor that utilizes a random forest with the same hyper parameters as above.
   * e. (1.5 points) For both models, calculate the MSE and the MAE, compare and discuss the results. Why does one model perform better for MSE and the other for MAE?
   * f. (1.5 points) Compute feature importances for both models. Which features are the most important in which models? Can you explain why?
4. **Out of Sample Prediction and Tree-based Models**: (1 point) Using the random forest without transformation you trained in the last problem, make an out of sample prediction for an observation $x$ where each feature's is greater than the max value of the features in `X_train`. Then make a prediction for $2 * x$ and $3 * x$. What do you observe? Can you explain it? What implications can you draw about the applicability of tree-based models extrapolation problems (i.e. timeseries)?
5. **Optimal Split**: (3 points) Implement a function called optimal_split that takes a feature matrix X and a target y and returns the optimal split (the index of the feature and the value at which the split occurs), i.e. the split that minimizes MSE. Write tests ( assert statements) that show that your function works. One of your tests should show that your algorithm works on a feature matrix with one feature, another should work on a feature matrix of two features. Your function should extend to arbitrarily many features.


**Bonus**:
(3 points) **Decision Tree**: Implement a decision tree in the style of Scikit-lean (fit/predict methods) using only Python, the standard library, numpy or scipy. For convenience, you may assume that there is no irreducible error in the dataset (i.e. there are no two observations that have the same feature values, but different target values). You may also implement the algorithm so that every terminal node has exactly one observation in it. Write tests to show that your model is working. If your dataset doesn’t have irreducible error, your train MSE should be 0. (Hint: One approach to this problem is to implement a recursive function, another approach is to use a while-loop. In either case, you’ll likely want to use optimal_split as a helper function and perhaps modify it to also return the right and left datasets.)
